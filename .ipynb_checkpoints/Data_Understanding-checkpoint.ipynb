{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#Initialise Libraries\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import the initial training data\n",
    "training = pd.read_csv('cs-training.csv',index_col= 'id', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Discussion\n",
    "\n",
    "#### The training dataset has 150,000 observations, with the target being an indicator (i.e Y/N) to the individual experiencing 90 days past due delinquency or worse. \n",
    "\n",
    "** The features we will seek to make use of are:** \n",
    "> **RevolvingUtilizationOfUnsecuredLines** = Total balance on credit cards and personal lines of credit except real estate and no installment debt like car loans divided by the sum of credit limits. (percentage)\n",
    "\n",
    "> **age** = The age of borrower in years. (integer)\n",
    "\n",
    "> **NumberOfTime30-59DaysPastDueNotWorse** = Number of times borrower has been 30-59 days past due but no worse in the last 2 years. (integer)\n",
    "\n",
    "> **DebtRatio** = Monthly debt payments, alimony,living costs divided by monthy gross income. (percentage)\n",
    "\n",
    "> **MonthlyIncome** = Monthly income (real)\n",
    "\n",
    "> **NumberOfOpenCreditLinesAndLoans** = Number of Open loans (installment like car loan or mortgage) and Lines of credit (e.g. credit cards). (integer)\n",
    "\n",
    "> **NumberOfTimes90DaysLate** = Number of times borrower has been 90 days or more past due. (integer)\n",
    "\n",
    "> **NumberRealEstateLoansOrLines** = Number of mortgage and real estate loans including home equity lines of credit. (integer)\n",
    "\n",
    "> **NumberOfTime60-89DaysPastDueNotWorse** = Number of times borrower has been 60-89 days past due but no worse in the last 2 years. (integer) \n",
    "\n",
    "> **NumberOfDependents** = Number of dependents in family excluding themselves (spouse, children etc.). (integer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lets get a feel for all the columns in the dataframe\n",
    "training.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count the number and percentage of Serious Deliquencies in the training set\n",
    "print \"The number of Serious Deliquencies in the training set is\" , training.SeriousDlqin2yrs.sum()\n",
    "print \"The percentage of deliquencies in the training set is \", \\\n",
    "float(sum(training.SeriousDlqin2yrs == 1) * 100) / float(training.SeriousDlqin2yrs.count()) , \"%\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "> So the training data is quite unbalanced. This is to a large extent expected given the nature of the problem. This will cause issues even with some very good learning algorithms unless we carefully control for this imbalance. If we dont, the learner will often consider this as noise . We may still end up with a high accuracy, but low sensitivity. Ultimately we want to be able to predict the number the individuals who will be under financial distress, as these are the applications which we are more likely to refuse as a bank, as this threatens the Banks' liquidity position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Check for missing values also. This will present an issue later when we attempt to applying modelling frameworks. \n",
    "#So we better check now.  \n",
    "training.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "> The training data has no missing values for almost all variables except for the monthly income and also the number of dependants. The monthly income variable will need to be treated with caution. Here we need to ask ourselves, what is the data generating process? The data would most likely be gathered from loan applications. This would presumably be a mandatory field. So there is data loss which we will need to account for. As one would expect monthly income to have a strong relationship with the rate of delinquency, we will need to infer the monthly income somehow. If we consider the variable 'Debt Ratio', this is \"Monthly debt payments, alimony, living costs divided by monthy gross income\". The data here is full, so we may be able to use this variable to impute values for monthly income, based on this ratio the like ratios of individuals and their reported incomes. The number of dependents also presents an issue for a learning algorithm. Using some common sense, we could argue that the field may not have been filled out because the individual may not have dependents. So it could be argued that ever missing value is \"0\". Before such a crude rule is applied, we may decide to cluster individuals to see if we can find commonalities. \n",
    "\n",
    "*** (nb). Some leaners can handle missing data. For instance, classification and regression trees can deal with missing data by using surrogate splits. However the scikit learn implementation does not support this so we will be explicity controlling for missing data in all models. ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Given all variables are numeric, we will look at the correlations in the data and histograms.\n",
    "#training.corr(method='kendall')  #non-parametric method .. (nb) Takes longer to run\n",
    "training.corr(method='pearson') #simple linear correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation\n",
    "\n",
    "One of the key challenges of data analysis is to be able to extract meaningful visualisations out of the data. Considering that we are dealing with a classification problem and there are 10 features, we will consider how the data appears in a lower dimensional space. We'll perform singular value decomposition to analyse the directions which we should perform othogonal projections.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
